{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<a href=\"http://www.insa-toulouse.fr/\" ><img src=\"http://www.math.univ-toulouse.fr/~besse/Wikistat/Images/logo-insa.jpg\" style=\"float:left; max-width: 120px; display: inline\" alt=\"INSA\"/></a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Machine Learning Tutorial](https://github.com/wikistat/MLTraining): IoT and Human Activity Recognition (HAR)\n",
    "## Analyse  de signaux  issus d'un *smartphone* \n",
    "## Utilisation des librairies <a href=\"http://scikit-learn.org/stable/#\"><img src=\"http://scikit-learn.org/stable/_static/scikit-learn-logo-small.png\" style=\"max-width: 100px; display: inline\" alt=\"Scikit-Learn\"/></a> en <a href=\"https://www.python.org/\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/f/f8/Python_logo_and_wordmark.svg/390px-Python_logo_and_wordmark.svg.png\" style=\"max-width: 120px; display: inline\" alt=\"Python\"/></a> et <a href=\"https://keras.io/\"><img src=\"https://s3.amazonaws.com/keras.io/img/keras-logo-2018-large-1200.png\" style=\"max-width: 100px; display: inline\" alt=\"Keras\"/></a> \n",
    "\n",
    "\n",
    "### Résumé\n",
    "Cas d'usage de [reconnaissance d'activités humaines](https://archive.ics.uci.edu/ml/datasets/Human+Activity+Recognition+Using+Smartphones) à partir des enregistrements de signaux (gyroscope, accéléromètre) issus d'un *objet connecté*: un simple smartphone. Les données sont analysées pour illustrer les principales étapes communes en *science des données* et appliquables à des signaux physiques échantillonnés. Visualisation des signaux bruts afin d'évaluer les difficultés posées par ce type de données; exploration ([analyse en composantes principales](http://wikistat.fr/pdf/st-m-explo-acp.pdf), [analyse factorielle discriminante](http://wikistat.fr/pdf/st-m-explo-acp.pdf)) des données transformées (*features*) ou *métier* calculées à partir des signaux; prévision de l'activité à partir des données métier par la plupart des méthodes linéaires dont: [régression logistique](http://wikistat.fr/pdf/st-m-app-rlogit.pdf), [SVM](http://wikistat.fr/pdf/st-m-app-svm.pdf) et non linéaires; prévision de l'activité à partir des signaux bruts par [réseau de neurones](http://wikistat.fr/pdf/st-m-app-rn.pdf) de type perceptron multicouches. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  1 Introduction\n",
    "### 1.1  Objectif général\n",
    "L'objectif est de reconnaître l'activité d'un individu porteur d'un smartphone qui enregistre un ensemble de signaux issus du gyroscope et de l'accéléromètre embarqués et ainsi connectés. Une base de données d'apprentissage a été construite expérimentalement. Un ensemble de porteurs d'un smartphone ont produit une activité déterminée pendant un laps de temps prédéfini tandis que des signaux étaient enregistrés. Les données sont issues de la communauté qui vise la reconnaissance d'activités humaines (*Human activity recognition, HAR*). Voir à ce propos l'[article](https://www.elen.ucl.ac.be/Proceedings/esann/esannpdf/es2013-11.pdf) relatant un colloque de 2013.  L'analyse des données associée à une identification d'activité en temps réel, ne sont pas abordées.\n",
    "\n",
    "Les données publiques disponibles ont été acquises, décrites et partiellement analysées par [Anguita et al. (2013)](https://www.icephd.org/sites/default/files/IWAAL2012.pdf). Elles sont accessibles sur le [dépôt](https://archive.ics.uci.edu/ml/datasets/Human+Activity+Recognition+Using+Smartphones) de l'University California Irvine (UCI) consacré à l'apprentissage machine.\n",
    "\n",
    "L'archive contient les données brutes: accélérations échantillonnnées à 64 htz pendant 2s. Les accélérations en x, y, et z, chacune de 128 colonnes, celles en y soustrayant la gravité naturelle ainsi que les accélérations angulaires (gyroscope) en x, y, et z soit en tout 9 fichiers. Le choix d'une puissance de 2 pour la fréquence d'échantillonnage permet l'exécution efficace d'algorithmes de transformée de Fourier ou en ondelettes.\n",
    "\n",
    "\n",
    "### 1.2 Déroulement\n",
    "Une première visualisation et exploration des signaux bruts montre (section 2) que ceux-ci sont difficiles à analyser; les classes d'activité y sont en effet mal caractérisées. La principale cause est l'absence de synchronisation des débuts d'activité; le déphasage des signaux apparaît alors comme un bruit ou artefact très préjudiciable à la bonne discrimination des activités sur la base d'une distance euclidienne usuelle ($L_2$). C'est la raison pour laquelle, [Anguita et al. (2013)](https://www.icephd.org/sites/default/files/IWAAL2012.pdf) proposent de calculer un ensemble de transformations ou caractéristiques (*features*) classiques du traitement du signal: variance, corrélations, entropie, décompositions de Fourier... Ce sont alors $p=561$ variables qui sont considérées et explorées dans la section 3. L'[analyse en composantes principales](http://wikistat.fr/pdf/st-m-explo-acp.pdf) et surtout l'[analyse factorielle discriminante](http://wikistat.fr/pdf/st-m-explo-acp.pdf) montrent les bonnes qualités discriminatoires de ces données \"métier\" issues d'une connaissance experte des signaux. La section 4 exploite ces variables métier et montre que des modèles statistiques élémentaires car linéaires (régression logistique, analyse discriminante) ou qu'un algorithme classique de machine à vecteurs supports (SVM) utilisant un simple noyau linéaire conduisent à d'excellentes prévisions au contraire d'algorihtmes non linaires sophistiqués (*random forest*).\n",
    "\n",
    "Néanmoins, faire calculer en permanence des transformations sophistiquées  n'est pas une solution viable pour la batterie d'un objet embarqué connecté. L'algorithme candidat doit pouvoir produire une solution intégrable (cablée) dans un cicuit, comme c'est par exemple le cas des puces dédiées à la reconnaissance faciale. C'est l'objet de la section 5: montrer la faisabilité d'une solution basée sur les seuls signaux bruts; solution mettant en oeuvre des réseaux de neurones.\n",
    "\n",
    "\n",
    "### 1.3 Environnement logiciel\n",
    "Pour être exécuté, ce calepin (*jupyter notebook*) nécessite l'installation de Python 3 via par exemple le site  [Anaconda](https://conda.io/docs/user-guide/install/download.html). Les algorihtmes d'exploration et d'apprentissage statistique utilisés sont disponibles dans la librairie [`Scikit-learn`](http://scikit-learn.org/stable/). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <FONT COLOR=\"Red\">Episode 1</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Etude préalable des signaux bruts\n",
    "### 2.1 Source\n",
    "\n",
    "Les données sont celles  du dépôt de l'[UCI](https://archive.ics.uci.edu/ml/datasets/Human+Activity+Recognition+Using+Smartphones). Elle peuvent  être préalablement téléchargées en cliquant [ici](https://archive.ics.uci.edu/ml/machine-learning-databases/00240/UCI%20HAR%20Dataset.zip).\n",
    "\n",
    "Chaque enregistrement ou unité statistique ou instance est labellisée avec **6 activités**: debout, assis, couché, marche, monte ou descend un escalier. Chaque jeu de données est partagé en une partie échantillon d'apprentissage et une partie échantillon test. L'échantillon test n'est utilisé que pour évaluer et comparer les qualités de prévision des principales méthodes. Il est conservé en l'état afin de rendre les comparaisons possibles avec les résultats de la littérature. Il s'agit donc d'un problème de *classification supervisée* (6 classes) avec $n=10299$ échantillons pour l'apprentissage, 2947 pour le test.\n",
    "\n",
    "Les données contiennent deux jeux de dimensions différentes:\n",
    "\n",
    "1. Jeu multidimensionel: un individus est constitué de 9 Séries Temporelles de *dimensions* $(n, 128, 9)$.\n",
    "2. Jeu unidimensionnel: Les 9 Séries Temporelles sont concaténées pour constituer un vecteur de 128*9 = 1152 variables de *dimensions* $(n, 1152)$.\n",
    "\n",
    "*N.B.* La structure des données est nettement plus complexe que celles couramment étudiées dans le [dépôt Wikistat](https://github.com/wikistat/). Le code a été structuré en une séquence de fonctions afin d'en faciliter la compréhension. L'outil *calepin* atteint ici des limites pour la réalisation de codes complexes.\n",
    "\n",
    "\n",
    "### 2.2 Importation des principales librairies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T09:09:21.403179Z",
     "start_time": "2020-04-14T09:09:20.486909Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import copy\n",
    "import random\n",
    "import itertools\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Structurer les données\n",
    "Définir le chemin d'accès aux données puis les fonctions utiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T09:09:21.419530Z",
     "start_time": "2020-04-14T09:09:21.405490Z"
    }
   },
   "outputs": [],
   "source": [
    "# Attention: le chemin ci-dessous doit être adapté au contexte\n",
    "DATADIR_UCI = './UCI HAR Dataset'\n",
    "# Liste des noms des fichiers afin d'automatiser la lecture.\n",
    "SIGNALS = [ \"body_acc_x\", \"body_acc_y\", \"body_acc_z\", \"body_gyro_x\", \"body_gyro_y\", \"body_gyro_z\", \"total_acc_x\", \"total_acc_y\", \"total_acc_z\"]\n",
    "\n",
    "# Fonctions permettant de lire la séquence des fichiers avant de restructurer les données \n",
    "# dans le fortmat recherché.\n",
    "def my_read_csv(filename):\n",
    "    return pd.read_csv(filename, delim_whitespace=True, header=None)\n",
    "\n",
    "def load_signal(data_dir, subset, signal):\n",
    "    filename = data_dir+'/'+subset+'/Inertial Signals/'+signal+'_'+subset+'.txt'\n",
    "    x = my_read_csv(filename).values\n",
    "    return x \n",
    "\n",
    "def load_signals(data_dir, subset, flatten = False):\n",
    "    signals_data = []\n",
    "    for signal in SIGNALS:\n",
    "        signals_data.append(load_signal(data_dir, subset, signal)) \n",
    "    if flatten :\n",
    "        X = np.hstack(signals_data)\n",
    "    else:\n",
    "        X = np.transpose(signals_data, (1, 2, 0))    \n",
    "    return X \n",
    "\n",
    "def load_y(data_dir, subset, dummies = False):\n",
    "    filename = data_dir+'/'+subset+'/y_'+subset+'.txt'\n",
    "    y = my_read_csv(filename)[0]\n",
    "    if dummies:\n",
    "        Y = pd.get_dummies(y).values\n",
    "    else:\n",
    "        Y = y.values\n",
    "    return Y "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lecture des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T09:09:31.563886Z",
     "start_time": "2020-04-14T09:09:21.423726Z"
    }
   },
   "outputs": [],
   "source": [
    "#Multidimensional Data\n",
    "X_train, X_test = load_signals(DATADIR_UCI, 'train'), load_signals(DATADIR_UCI, 'test')\n",
    "# Flattened Data\n",
    "X_train_flatten, X_test_flatten = load_signals(DATADIR_UCI, 'train', flatten=True), load_signals(DATADIR_UCI, 'test', flatten=True)\n",
    "\n",
    "# Label Y\n",
    "Y_train_label, Y_test_label = load_y(DATADIR_UCI, 'train', dummies = False), load_y(DATADIR_UCI, 'test', dummies = False)\n",
    "#Dummies Y (For Keras)\n",
    "Y_train_dummies, Y_test_dummies = load_y(DATADIR_UCI, 'train', dummies = True), load_y(DATADIR_UCI, 'test', dummies = True)\n",
    "\n",
    "N_train = X_train.shape[0]\n",
    "N_test = X_test.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vérification des dimensions afin de s'assurer de la bonne lecture des fichiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T09:09:31.579575Z",
     "start_time": "2020-04-14T09:09:31.566388Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Dimension\")\n",
    "print(\"Données Multidimensionelles, : \" + str(X_train.shape))\n",
    "print(\"Données Unimensionelles, : \" + str(X_train_flatten.shape))\n",
    "print(\"Vecteur réponse (scikit-learn) : \" + str(Y_train_label.shape))\n",
    "print(\"Matrice réponse(Keras) : \" + str(Y_train_dummies.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Visualisations\n",
    "Cette phase est essentielle à la bonne compréhension des données, de leur structure et donc des problèmes qui vont être soulevés par la suite. La visualisation est très élémentaire d'un point de vue méthodologique mais nécessite des compétences plus élaborées en Python et donc des fonctions préalables. \n",
    "#### Fonctions utiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T09:09:31.609094Z",
     "start_time": "2020-04-14T09:09:31.584773Z"
    }
   },
   "outputs": [],
   "source": [
    "# Liste des couleurs\n",
    "CMAP = plt.get_cmap(\"Accent\")\n",
    "# Liste des types de signaux\n",
    "SIGNALS = [\"body_acc x\", \"body_acc y\", \"body_acc z\", \n",
    "                \"body_gyro x\", \"body_gyro y\", \"body_gyro z\", \n",
    "               \"total_acc x\", \"total_acc y\", \"total_acc z\"] \n",
    "# Dictionnaire en clair des activités expérimentées (contexte supervisé)\n",
    "ACTIVITY_DIC = {1 : \"WALKING\",\n",
    "2 : \"WALKING UPSTAIRS\",\n",
    "3 : \"WALKING DOWNSTAIRS\",\n",
    "4 : \"SITTING\",\n",
    "5 : \"STANDING\",\n",
    "6 : \"LAYING\"}\n",
    "labels = ACTIVITY_DIC.values()\n",
    "\n",
    "# Fonction pour le tracé d'un signal\n",
    "def plot_one_axe(X, fig, ax, sample_to_plot, cmap):\n",
    "    for act,Xgb in X.groupby(\"Activity\"):\n",
    "        Xgb_first_values = Xgb.values[:sample_to_plot,:-1]\n",
    "        x = Xgb_first_values[0]\n",
    "        ax.plot(x, linewidth=1, color=cmap(act-1), label = label_dic[act])\n",
    "        for x in Xgb_first_values[1:]:\n",
    "            ax.plot(x, linewidth=1, color=cmap(act-1))\n",
    "def plot_one_axe_shuffle(X, fig, ax, sample_to_plot, cmap):\n",
    "    plot_data = []\n",
    "    for act,Xgb in X.groupby(\"Activity\"):\n",
    "        Xgb_first_values = Xgb.values[:sample_to_plot,:-1]\n",
    "        x = Xgb_first_values[0]\n",
    "        ax.plot(x, linewidth=1, color=cmap(act-1), label = label_dic[act])\n",
    "        for x in Xgb_first_values[1:]:\n",
    "            plot_data.append([x,cmap(act-1)])\n",
    "    random.shuffle(plot_data)\n",
    "    for x,color in plot_data:\n",
    "        ax.plot(x, linewidth=1, color=color)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Tracés de tous les signaux\n",
    "Tous les signaux sont tracés par type en superposant les activités."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T09:09:44.902940Z",
     "start_time": "2020-04-14T09:09:31.611691Z"
    }
   },
   "outputs": [],
   "source": [
    "sample_to_plot = 50\n",
    "index_per_act = [list(zip(np.repeat(act, sample_to_plot), np.where(Y_train_label==act)[0][:sample_to_plot])) for act in range(1,7)]\n",
    "index_to_plot = list(itertools.chain.from_iterable(index_per_act))\n",
    "random.shuffle(index_to_plot)\n",
    "\n",
    "fig = plt.figure(figsize=(15,10))\n",
    "for isignal in range(9):\n",
    "    ax = fig.add_subplot(3,3,isignal+1)\n",
    "    for act , i in index_to_plot:\n",
    "        ax.plot(range(128), X_train[i,:,isignal],color=CMAP(act-1), linewidth=1)\n",
    "        ax.set_title(SIGNALS[isignal])\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remarque :** Apprécier la difficulté à distinguer les activités au sein d'un même signal.\n",
    "\n",
    "### 3.3 Par signal \n",
    "Le seul signal \"acélération en\" x est tracé en distinguant les activités."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T09:09:46.526860Z",
     "start_time": "2020-04-14T09:09:44.904853Z"
    }
   },
   "outputs": [],
   "source": [
    "sample_to_plot = 10\n",
    "isignal = 1\n",
    "index_per_act_dict = dict([(act, np.where(Y_train_label==act)[0][:sample_to_plot]) for act in range(1,7)])\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(15,8), num=SIGNALS[isignal])\n",
    "for act , index in index_per_act_dict.items():\n",
    "    ax = fig.add_subplot(3,2,act)\n",
    "    for x in X_train[index]:\n",
    "        ax.plot(range(128), x[:,0],color=CMAP(act-1), linewidth=1)\n",
    "    ax.set_title(ACTIVITY_DIC[act])\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q.** Quelle est l'activité qui semble se distinguer facilement des autres ? \n",
    "\n",
    "**Q.** Observer les signaux d'une activité, par exemple `Walking upstairs`. Qu'est-ce qui fait que, pour ces signaux ou courbes, une métrique Euclidienne classique ($L_2$) est inopérante ? \n",
    "\n",
    "C'est la raison pour laquelle il sera important de décomposer notamment les signaux dans le domaine des fréquences. \n",
    "\n",
    "\n",
    "### 3.4 [Analyse en composantes principales](http://wikistat.fr/pdf/st-m-explo-acp.pdf)\n",
    "Il est important de se faire une idée précise de la structure des données.  Une analyse en composantes principales est adaptée à cet objectif. \n",
    "#### Remarques\n",
    "   - L'ACP n'est pas réduite sur ces données car cette transformation est sans effet sur la piètre qualité des graphiques.\n",
    "   - L'ACP basée sur une métrique Euclidienne usuelle ne fait que confirmer les difficultés précedemment identifiées et l'absence de pouvoir discriminant des données brutes au sens de cette métrique; cette exploration n'est pas approfondies sur ces données. En revanche un autre [calepin](https://github.com/wikistat/Exploration/blob/master/HumanActivityRecognition/Explo-Python-Har-brutes.ipynb) détaille une analyse factorielle discriminante mais avec la même conclusion.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La fonction définie ci-après affiche un nuage de points dans un plan factoriel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T09:09:46.540744Z",
     "start_time": "2020-04-14T09:09:46.530594Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_pca(X_R, ytrain, fig, ax, nbc, nbc2, label_dic=ACTIVITY_DIC, cmaps = plt.get_cmap(\"Accent\")\n",
    "):\n",
    "    for i in range(6):\n",
    "        xs = X_R[ytrain==i+1,nbc-1]\n",
    "        ys = X_R[ytrain==i+1, nbc2-1]\n",
    "        label = label_dic[i+1]\n",
    "        color = cmaps(i)\n",
    "        ax.scatter(xs, ys, color=color, alpha=.8, s=10, label=label)\n",
    "        ax.set_xlabel(\"PC%d : %.2f %%\" %(nbc,pca.explained_variance_ratio_[nbc-1]*100), fontsize=15)\n",
    "        ax.set_ylabel(\"PC%d : %.2f %%\" %(nbc2,pca.explained_variance_ratio_[nbc2-1]*100), fontsize=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ACP sur un type de signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T09:09:47.938156Z",
     "start_time": "2020-04-14T09:09:46.548289Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA()\n",
    "# Choix du signal\n",
    "isignal = 4\n",
    "signal = SIGNALS[isignal]\n",
    "print(\"ACP Sur signal : \" +signal)\n",
    "X_c = pca.fit_transform(X_train[:,:,isignal])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Représentation des parts de variance expliquée par les premières composantes principales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T09:09:48.644737Z",
     "start_time": "2020-04-14T09:09:47.940412Z"
    }
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15,10))\n",
    "ax = fig.add_subplot(1,2,1)\n",
    "ax.bar(range(10), pca.explained_variance_ratio_[:10]*100, align='center',\n",
    "        color='grey', ecolor='black')\n",
    "ax.set_xticks(range(10))\n",
    "ax.set_ylabel(\"Variance\")\n",
    "ax.set_title(\"\", fontsize=35)\n",
    "ax.set_title(u\"Pourcentage de variance expliquée \\n par les premières composantes\", fontsize=20)\n",
    "\n",
    "ax = fig.add_subplot(1,2,2)\n",
    "box=ax.boxplot(X_c[:,0:10],whis=100)\n",
    "ax.set_title(u\"Distribution des premières composantes\", fontsize=20)\n",
    "\n",
    "fig.suptitle(u\"Résultat ACP sur Signal : \" + signal, fontsize=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Attention**: les diagrammes boîtes sont très perturbés par les distributions des composantes avec une très forte concentration autour de 0 et énormément de valeurs atypiques. D'où l'utilisation du paramètre `whis=100` pour rallonger les moustaches.\n",
    "\n",
    "**Q.** Que sont les graphes ci-dessus ? Quelles interprétations ou absence d'interprétation en tirer ?\n",
    "\n",
    "**Représentation du premier plan factoriel :**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T09:09:49.039696Z",
     "start_time": "2020-04-14T09:09:48.646489Z"
    }
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,10), )\n",
    "ax = fig.add_subplot(1,1,1)\n",
    "plot_pca(X_c, Y_train_label,fig ,ax ,1 ,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les autres plans factoriels ne sont guère plus informatifs.\n",
    "#### Sur tous les signaux\n",
    "Tous les signaux sont concaténés à plat en un seul signal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T09:09:52.091255Z",
     "start_time": "2020-04-14T09:09:49.041222Z"
    }
   },
   "outputs": [],
   "source": [
    "pca = PCA()\n",
    "print(\"ACP Sur tous les signaux\")\n",
    "X_c = pca.fit_transform(X_train_flatten)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T09:09:52.852861Z",
     "start_time": "2020-04-14T09:09:52.099072Z"
    }
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15,10))\n",
    "ax = fig.add_subplot(1,2,1)\n",
    "ax.bar(range(10), pca.explained_variance_ratio_[:10]*100, align='center',\n",
    "        color='grey', ecolor='black')\n",
    "ax.set_xticks(range(10))\n",
    "ax.set_ylabel(\"Variance\")\n",
    "ax.set_title(\"\", fontsize=35)\n",
    "ax.set_title(u\"Pourcentage de variance expliqué \\n des premières composantes\", fontsize=20)\n",
    "\n",
    "ax = fig.add_subplot(1,2,2)\n",
    "box=ax.boxplot(X_c[:,0:10],whis=100)\n",
    "ax.set_title(u\"Distribution des premières composantes\", fontsize=20)\n",
    "\n",
    "fig.suptitle(u\"Résultat ACP\", fontsize=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T09:09:53.240748Z",
     "start_time": "2020-04-14T09:09:52.854459Z"
    }
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,10), )\n",
    "ax = fig.add_subplot(1,1,1)\n",
    "plot_pca(X_c, Y_train_label,fig ,ax ,1 ,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q.** Quelle activité semble néanmoins facile à identifier ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Exploration des données \"métier\"\n",
    "### 3.1 Les données\n",
    "L'[archive de l'UCI]() contient également deux fichiers `train` et `test` des 561 caractéristiques (*features*) ou variables \"métier\" calculées dans les domaines temporels et fréquentiels par transformation des signaux bruts.\n",
    "\n",
    "Voici une liste indicative des variables calculées sur chacun des signaux bruts ou couples de signaux:\n",
    "\n",
    "Name|Signification\n",
    "-|-\n",
    "mean | Mean value\n",
    "std | Standard deviation\n",
    "mad | Median absolute value\n",
    "max | Largest values in array\n",
    "min | Smallest value in array\n",
    "sma | Signal magnitude area\n",
    "energy | Average sum of the squares\n",
    "iqr | Interquartile range\n",
    "entropy | Signal Entropy\n",
    "arCoeff | Autorregresion coefficients\n",
    "correlation | Correlation coefficient\n",
    "maxFreqInd | Largest frequency component\n",
    "meanFreq | Frequency signal weighted average\n",
    "skewness | Frequency signal Skewness\n",
    "kurtosis | Frequency signal Kurtosis\n",
    "energyBand | Energy of a frequency interval\n",
    "angle | Angle between two vectors\n",
    "\n",
    "#### Lecture des données métier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T09:09:54.976847Z",
     "start_time": "2020-04-14T09:09:53.243035Z"
    }
   },
   "outputs": [],
   "source": [
    "# Lecture des données d'apprentissage\n",
    "# Attention, il peut y avoir plusieurs espaces comme séparateur dans le fichier\n",
    "Xtrain=pd.read_csv(\"X_train.txt\",sep='\\s+',header=None)\n",
    "Xtrain.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T09:09:54.985255Z",
     "start_time": "2020-04-14T09:09:54.979066Z"
    }
   },
   "outputs": [],
   "source": [
    "# Variable cible\n",
    "ytrain=pd.read_csv(\"y_train.txt\",sep='\\s+',header=None,names=['y'])\n",
    "# Le type dataFrame est inutile et même gênant pour la suite\n",
    "ytrain=ytrain[\"y\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T09:09:55.696175Z",
     "start_time": "2020-04-14T09:09:54.987714Z"
    }
   },
   "outputs": [],
   "source": [
    "# Lecture des données de test\n",
    "Xtest=pd.read_csv(\"X_test.txt\",sep='\\s+',header=None)\n",
    "Xtest.shape\n",
    "ytest=pd.read_csv(\"y_test.txt\",sep='\\s+',header=None,names=['y'])\n",
    "ytest=ytest[\"y\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 [Analyse en composantes principales](http://wikistat.fr/pdf/st-m-explo-acp.pdf)\n",
    "Fonction graphique pour les plans factoriels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T09:09:55.705248Z",
     "start_time": "2020-04-14T09:09:55.699148Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_pca(X_R,fig,ax,nbc,nbc2):\n",
    "    for i in range(6):\n",
    "        xs = X_R[ytrain==i+1,nbc-1]\n",
    "        ys = X_R[ytrain==i+1, nbc2-1]\n",
    "        label = ACTIVITY_DIC [i+1]\n",
    "        color = cmaps(i)\n",
    "        ax.scatter(xs, ys, color=color, alpha=.8, s=1, label=label)\n",
    "        ax.set_xlabel(\"PC%d : %.2f %%\" %(nbc,pca.explained_variance_ratio_[nbc-1]*100), fontsize=10)\n",
    "        ax.set_ylabel(\"PC%d : %.2f %%\" %(nbc2,pca.explained_variance_ratio_[nbc2-1]*100), fontsize=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calcul de la matrice des composantes principales. C'est aussi un changement (transformation) de base; de la base canonique dans la base des vecteurs propres. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T09:09:56.414735Z",
     "start_time": "2020-04-14T09:09:55.708292Z"
    }
   },
   "outputs": [],
   "source": [
    "pca = PCA()\n",
    "X_c = pca.fit_transform(Xtrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Valeurs propres ou variances des composantes principales\n",
    "Représentation de la décroissance des valeurs propres, les variances des variables ou composantes principales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T09:09:56.688255Z",
     "start_time": "2020-04-14T09:09:56.418044Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(pca.explained_variance_ratio_[0:10])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un graphique plus explicite décrit les distribution de ces composantes par des diagrames boîtes; seules les premières sont affichées. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T09:09:57.183945Z",
     "start_time": "2020-04-14T09:09:56.691838Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.boxplot(X_c[:,0:10])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Commenter la décroissance des variances, le choix éventuel d'une dimension ou nombre de composantes à retenir sur les 561.\n",
    "#### Représentation des individus ou \"activités\" en ACP\n",
    "Projection dans les principaux plans factoriels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T09:09:59.523613Z",
     "start_time": "2020-04-14T09:09:57.186510Z"
    }
   },
   "outputs": [],
   "source": [
    "cmaps = plt.get_cmap(\"Accent\")\n",
    "fig = plt.figure(figsize= (20,20))\n",
    "count = 0\n",
    "for nbc, nbc2,count in [(1,2,1), (2,3,2), (3,4,3), (1,3,4), (2,4,5), (1,4,7)] :\n",
    "    ax = fig.add_subplot(3,3,count)\n",
    "    plot_pca(X_c, fig,ax,nbc,nbc2)\n",
    "plt.legend(loc='upper right', bbox_to_anchor=(1.8, 0.5), markerscale=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q.** Commenter la séparation des deux types de situation par le premier axe.\n",
    "\n",
    "**Q.** Que dire sur la forme des nuages ?\n",
    "\n",
    "**Q.** Que dire sur la plus ou moins bonne séparation des classes ?\n",
    "#### Représentation des variables en ACP\n",
    "Lecture des libellés des variables et constitution d'une liste. Souci de la grande dimension (561), les représentations ne sont guère exploitables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T09:09:59.542767Z",
     "start_time": "2020-04-14T09:09:59.530303Z"
    }
   },
   "outputs": [],
   "source": [
    "with open('features.txt', 'r') as content_file:\n",
    "    featuresNames = content_file.read()\n",
    "columnsNames = list(map(lambda x : x.split(\" \")[1],featuresNames.split(\"\\n\")[:-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Graphe des variables illisible en mettant les libellés en clair. Seule une * est représentée."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T09:10:03.018314Z",
     "start_time": "2020-04-14T09:09:59.546646Z"
    }
   },
   "outputs": [],
   "source": [
    "# coordonnées des variables\n",
    "coord1=pca.components_[0]*np.sqrt(pca.explained_variance_[0])\n",
    "coord2=pca.components_[1]*np.sqrt(pca.explained_variance_[1])\n",
    "fig = plt.figure(figsize=(8,8))\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "for i, j in zip(coord1,coord2, ):\n",
    "    plt.text(i, j, \"*\")\n",
    "    plt.arrow(0,0,i,j,color='r')\n",
    "plt.axis((-1.2,1.2,-1.2,1.2))\n",
    "# cercle\n",
    "c=plt.Circle((0,0), radius=1, color='b', fill=False)\n",
    "ax.add_patch(c)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identification des variables participant le plus au premier axe. Ce n'est pas plus clair! Seule la réprésentation des individus apporte finalement des éléments de compréhension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T09:10:03.026378Z",
     "start_time": "2020-04-14T09:10:03.020595Z"
    }
   },
   "outputs": [],
   "source": [
    "print(np.array(columnsNames)[abs(coord1)>.6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 [Analyse Factorielle Discriminante (AFD)](http://wikistat.fr/pdf/st-m-explo-afd.pdf)\n",
    "#### Principe\n",
    "L'ACP ne prend pas en compte la présence de la variable qualitative à modéliser contrairement à l'analyse factorielle discriminante (AFD) adaptés à ce contexte \"supervisé\" puisque l'activité est connue sur un échantillon d'apprentissage. L'AFD est une ACP des barycentres des classes munissant l'espace des individus d'une métrique spécifique dite de *Mahalanobis*. Métrique définie par l'inverse de la matrice de covariance intraclase. L'objectif est alors de visualiser les capacités des variables à discriminer les classes.\n",
    "\n",
    "La librairie `scikit-learn` ne propose pas de fonction spécifique d'analyse factorielle discriminante mais les coordonnées des individus dans la base des vecteurs discriminants sont obtenues comme résultats de l'analyse discriminante linéaire décisionnnelle. Cette dernière sera utilisée avec une finalité prédictive dans un deuxième temps (autre calepin). \n",
    "\n",
    "Les résultats de la fonction `LinearDiscriminantAnalysis` de `scikit-learn` sont identiques à ceux de la fonction `lda` de R. Elle est donc utilisée strictement de la même façon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T09:10:04.460102Z",
     "start_time": "2020-04-14T09:10:03.032128Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "method = LinearDiscriminantAnalysis() \n",
    "lda=method.fit(Xtrain,ytrain)\n",
    "X_r2=lda.transform(Xtrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q.** Que signifie le *warning* ? Quel traitement faudrait-t-il mettre en oeuvre pour utiliser une analyse discriminante décisionnelle en modélisation ou apprentissage ?\n",
    "\n",
    "#### Représentation des individus en AFD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T09:10:06.668576Z",
     "start_time": "2020-04-14T09:10:04.464352Z"
    }
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize= (20,20))\n",
    "count = 0\n",
    "for nbc, nbc2,count in [(1,2,1), (2,3,2), (3,4,3), (1,3,4), (2,4,5), (1,4,7)] :\n",
    "    ax = fig.add_subplot(3,3,count)\n",
    "    plot_pca(X_r2, fig,ax,nbc,nbc2)\n",
    "plt.legend(loc='upper right', bbox_to_anchor=(1.8, 0.5), markerscale=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q.** Que dire de la séparation des classes ? Sont-elles toutes séparables deux-à-deux ?\n",
    "\n",
    "**Q.** Que dire de la forme des nuages notamment dans le premier plan ?\n",
    "\n",
    "Comme pour l'ACP, la représentation trop complexe des variables n'apporterait rien."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 [Classification non supervisée](http://wikistat.fr/pdf/st-m-explo-classif.pdf)\n",
    "Cette section n'est pas utile puisque les classes sont connues. Néanmoins, une approche générale des l'étude de signaux relatant des activités humaines non identifiées *a priori* nécessiterait cette phase de classification non supervisée ou *clustering*. Cette étape permet simplement ici  d'illustrer le comportement d'un algorithme de classification non supervisée classique. La matrice de confusion des classes obtenues avec celles connues permet d'en évaluer les performances. \n",
    "#### $k$*-means*\n",
    "Attention, il est nécessaire de centrer et réduire les variables avant d'exécuter un algorithme de classification non supervisé."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T09:10:11.356260Z",
     "start_time": "2020-04-14T09:10:06.670363Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "tps1 = time.perf_counter()\n",
    "X = StandardScaler().fit_transform(Xtrain)\n",
    "km=KMeans(n_clusters=6)\n",
    "km.fit(Xtrain)\n",
    "tps2 = time.perf_counter()\n",
    "print(\"Temps execution Kmeans :\", (tps2 - tps1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T09:10:11.426455Z",
     "start_time": "2020-04-14T09:10:11.360623Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "pd.DataFrame(confusion_matrix(ytrain, km.labels_)[1:7,0:6].T, columns=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q.** Que dire de l'efficacité d'une approche non supervisée pour catégoriser les activités ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <FONT COLOR=\"Red\">Episode 2</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Prévision de l'activité à partir des variables \"métier\"\n",
    "Plusieurs méthodes sont successivement testées dans ce calepin : SVM, analyse discriminate décisionnelle, $k$ plus proches voisins, forêts aléatoires, réseaux de neurones... Nous commençons par la régression logistique  pour la prévision du comportement.\n",
    "\n",
    "### 4.1 [Régression logistique](http://wikistat.fr/pdf/st-m-app-rlogit.pdf)\n",
    "\n",
    "####  Principe\n",
    "Une méthode statistique ancienne mais finalement efficace sur ces données. La régression logistique est adaptée à la prévision d'une variable binaire. Dans le cas multiclasse, la fonction logistique de la librairie `Scikit-learn` estime *par défaut* **un modèle par classe**: une classe contre les autres. \n",
    "\n",
    "La probabilité d'appartenance d'un individu à une classe est modélisée à l'aide d'une combinaison linéaire des variables explicatives. Pour transformer une combinaison linéaire à valeur dans $R$ en une probabilité à valeurs dans l'intervalle $[0, 1]$, une fonction de forme sigmoïdale est appliquée.  Ceci donne: $$P(y_i=1)=\\frac{e^{Xb}}{1+e^{Xb}}$$ ou, c'est équivalent, une décomposition linéaire du *logit* ou *log odd ratio* de  $P(y_i=1)$:  $$\\log\\frac{P(y_i=1)}{1-P(y_i=1)}=Xb.$$\n",
    "\n",
    "\n",
    "####  Estimation du modèle sans optimisation\n",
    "Le modèle est estimé sans chercher à raffiner les valeurs de certains paramètres (pénalisation). Ce sera fait dans un deuxième temps. Le paramètre de choix du *solver* est précisé car le choix par défaut (`lbfgs`) semble converger moins vite. Une comparaison systématique des différentes options (`liblinear, lbfgs, saga, sag, newton-cg`) serait bienvenue en association avec le choix de modèle lorsque le nombre de classes est plus grand que 2: fonction perte multinomiale ou un modèle binomial par classe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T09:10:18.398790Z",
     "start_time": "2020-04-14T09:10:11.430699Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "ts = time.time()\n",
    "method = LogisticRegression(solver='liblinear',multi_class='auto')\n",
    "method.fit(Xtrain,ytrain)\n",
    "score = method.score(Xtest, ytest)\n",
    "ypred = method.predict(Xtest)\n",
    "te = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prévision de l'activité de l'échantillon test\n",
    "Une fois le modèle estimé, l'erreur de prévision est évaluée, sans biais optimiste, sur un autre échantillon, dit échantillon test, qui n'a pas participé à l'apprentissage du modèle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T09:10:18.424156Z",
     "start_time": "2020-04-14T09:10:18.400699Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "print(\"Score : %f, time running : %d secondes\" %(score, te-ts))\n",
    "pd.DataFrame(confusion_matrix(ytest, ypred), index = labels, columns=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q.** Quelles sont les classes qui restent difficiles à discriminer ?\n",
    "\n",
    "**Q.** Commenter la qualité des résultats obtenus. Sont-ils cohérents avec l'approche exploratoire ?\n",
    "\n",
    "#### Optimisation du modèle par pénalisation Lasso\n",
    "*Attention* l'exécution est un peu longue... cette optimisation peut être omise en première lecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T09:19:19.967342Z",
     "start_time": "2020-04-14T09:10:18.428545Z"
    }
   },
   "outputs": [],
   "source": [
    "# Optimisation du paramètre de pénalisation\n",
    "# grille de valeurs\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "ts = time.time()\n",
    "param=[{\"C\":[0.5,1,5,10,12,15,30]}]\n",
    "logit = GridSearchCV(LogisticRegression(penalty=\"l1\",solver='liblinear', \n",
    "                                        multi_class='auto'), param,cv=10,n_jobs=-1)\n",
    "logitOpt=logit.fit(Xtrain, ytrain)  \n",
    "# paramètre optimal\n",
    "logitOpt.best_params_[\"C\"]\n",
    "te = time.time()\n",
    "print(\"Temps : %d secondes\" %(te-ts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T09:19:19.973912Z",
     "start_time": "2020-04-14T09:19:19.969854Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Meilleur score = %f, Meilleur paramètre = %s\" % (logitOpt.best_score_,logitOpt.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T09:19:19.995153Z",
     "start_time": "2020-04-14T09:19:19.976014Z"
    }
   },
   "outputs": [],
   "source": [
    "yChap = logitOpt.predict(Xtest)\n",
    "# matrice de confusion\n",
    "logitOpt.score(Xtest, ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T09:19:20.021501Z",
     "start_time": "2020-04-14T09:19:19.996900Z"
    }
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(confusion_matrix(ytest, yChap), index = labels, columns=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q.** L'amélioration est-elle bien significative au regard du temps de calcul ?\n",
    "\n",
    "**Q.** Déterminer les variables sélectionnées par la méthode LASSO. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 [Analyse discriminante linéaire](http://wikistat.fr/pdf/st-m-app-add.pdf)\n",
    "**Q.** Que dire de l'optimisation de cette méthode ? Celle-ci est proposée dans une librairie de R mais pas disponible en Python.\n",
    "\n",
    "**Q.** L'analyse discriminante quadratique pose quelques soucis. Pourquoi ?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T09:19:21.325921Z",
     "start_time": "2020-04-14T09:19:20.024699Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\n",
    "ts = time.time()\n",
    "method = LinearDiscriminantAnalysis()\n",
    "method.fit(Xtrain,ytrain)\n",
    "score = method.score(Xtest, ytest)\n",
    "ypred = method.predict(Xtest)\n",
    "te = time.time()\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T09:19:21.356007Z",
     "start_time": "2020-04-14T09:19:21.327902Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Score : %f, time running : %d secondes\" %(score, te-ts))\n",
    "pd.DataFrame(confusion_matrix(ytest, ypred), index = labels, columns=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 [*K* plus proches voisins](http://wikistat.fr/pdf/st-m-app-add.pdf)\n",
    "\n",
    "Cette méthode peut être vue comme un cas particulier d'analyse discriminante avec une estimation locale des fonctions de densité conditionnelle. \n",
    "\n",
    "**Q.** Combien de voisins sont utilisés par défaut ? Optimiser ce paramètre. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T09:19:39.487153Z",
     "start_time": "2020-04-14T09:19:21.357706Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "ts = time.time()\n",
    "method = KNeighborsClassifier(n_jobs=-1)\n",
    "method.fit(Xtrain,ytrain)\n",
    "score = method.score(Xtest, ytest)\n",
    "ypred = method.predict(Xtest)\n",
    "te = time.time()\n",
    "t_total = te-ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T09:19:39.508495Z",
     "start_time": "2020-04-14T09:19:39.488760Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Score : %f, time running : %d secondes\" %(score, te-ts))\n",
    "pd.DataFrame(confusion_matrix(ytest, ypred), index = labels, columns=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 [SVM linéaire](http://wikistat.fr/pdf/st-m-app-svm.pdf)\n",
    "\n",
    "\n",
    "Le nombre max d'itérations a été très sensiblement augmenté (1000 par défaut) sans pour autant améliorer la performance. Le cas multi-classes est traité en considérant une classe contre les autres donc 6 modèles.\n",
    "\n",
    "**Q.** Utiliser le paramètre par défaut puis faire varier le paramètre de pénalisation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T09:20:08.714818Z",
     "start_time": "2020-04-14T09:19:39.511096Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "ts = time.time()\n",
    "method = LinearSVC(max_iter=20000)\n",
    "method.fit(Xtrain,ytrain)\n",
    "score = method.score(Xtest, ytest)\n",
    "ypred = method.predict(Xtest)\n",
    "te = time.time()\n",
    "t_total = te-ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T09:20:08.740091Z",
     "start_time": "2020-04-14T09:20:08.716703Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(\"Score : %f, time running : %d secondes\" %(score, te-ts))\n",
    "pd.DataFrame(confusion_matrix(ytest, ypred), index = labels, columns=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 [SVM avec noyau gaussien](http://wikistat.fr/pdf/st-m-app-svm.pdf)\n",
    "\n",
    "Apprentissage avec les valeurs par défaut puis optimisation des paramètres.\n",
    "\n",
    "**Q.** Quels sont les paramètres à optimiser ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T09:20:36.973697Z",
     "start_time": "2020-04-14T09:20:08.742383Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "ts = time.time()\n",
    "method = SVC(gamma='auto')\n",
    "method.fit(Xtrain,ytrain)\n",
    "score = method.score(Xtest, ytest)\n",
    "ypred = method.predict(Xtest)\n",
    "te = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T09:20:36.992066Z",
     "start_time": "2020-04-14T09:20:36.975386Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Score : %f, time running : %d secondes\" %(score, te-ts))\n",
    "pd.DataFrame(confusion_matrix(ytest, ypred), index = labels, columns=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q.** Quelle procédure est exécutée ci-après et dans quel but ?\n",
    "\n",
    "*Attention*: l'exécution est un peu longue et peut être omise en preière lecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T09:24:44.746208Z",
     "start_time": "2020-04-14T09:20:36.993400Z"
    }
   },
   "outputs": [],
   "source": [
    "ts = time.time()\n",
    "param=[{\"C\":[4,5,6],\"gamma\":[.01,.02,.03]}]\n",
    "svm= GridSearchCV(SVC(),param,cv=10,n_jobs=-1)\n",
    "svmOpt=svm.fit(Xtrain, ytrain)\n",
    "te = time.time()\n",
    "te-ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T09:24:44.761547Z",
     "start_time": "2020-04-14T09:24:44.756680Z"
    }
   },
   "outputs": [],
   "source": [
    "# paramètre optimal\n",
    "print(\"Meilleur score = %f, Meilleur paramètre = %s\" % (svmOpt.best_score_,svmOpt.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q.** Comparer les deux approches par SVM (linéaire et radiale): temps de calcul et performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6 [*Random forest*](http://wikistat.fr/pdf/st-m-app-agreg.pdf)\n",
    "\n",
    "**Q.** Quel serait le paramètre à optimiser ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T09:24:58.273566Z",
     "start_time": "2020-04-14T09:24:44.764426Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "ts = time.time()\n",
    "method = RandomForestClassifier(n_estimators=200,n_jobs=-1)\n",
    "method.fit(Xtrain,ytrain)\n",
    "score = method.score(Xtest, ytest)\n",
    "ypred = method.predict(Xtest)\n",
    "te = time.time()\n",
    "t_total = te-ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T09:24:58.291331Z",
     "start_time": "2020-04-14T09:24:58.275291Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Score : %f, time running : %d secondes\" %(score, te-ts))\n",
    "pd.DataFrame(confusion_matrix(ytest, ypred), index = labels, columns=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.8 Combinaison de modèles\n",
    "Les formes des nuages de chaque classe observées dans le premier plan de l'analyse en composantes principales montrent que la structure de covariance n'est pas identique dans chaque classe. Cette remarque suggèrerait de s'intéresser à l'analyse discriminante quadratique mais celle-ci bloque sur l'estimation des six matrices de covariance et de leur inverse. Néanmoins il semble que, plus précisément, deux groupes se distinguent : les classes actives (marcher, monter ou descendre un escalier) d'une part et les classes passives (couché, assis, debout) d'autre part et, qu'à l'intérieur de chaque groupe les variances sont assez similaires. \n",
    "\n",
    "Cette situation suggère de construire une décision en deux étapes ou hiérarchique:\n",
    "1. Régression logistique séparant les activités passives *vs.* actives,\n",
    "2. Un modèle spécifique à chacune des classes précédentes, par exemple des SVM à noyau gaussien.\n",
    "\n",
    "Une telle construction hiérarchique de modèles aboutit à une précision supérieure à 97%.\n",
    "\n",
    "**Exercice :** Programmer une telle approche en utilisant les capacités de python pour réaliser un *pipeline*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <FONT COLOR=\"Red\">Episode 3</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 Prévision de l'activité à partir des signaux bruts\n",
    "### 5.1 Introduction\n",
    "Comme expliqué en introduction, le calcul des nombreuses transformations des données est bien trop consommateur des ressources de la batterie d'un objet connecté. Cette section se propose d'utiliser les seuls signaux bruts pour faire apprendre un algorithme et parmi les algorithmes possibles, seuls les réseaux de neurones pouvant être \"cablés\" dans un circuit sont pris en compte. Nous utilisons ici un réseau de type perceptron multicouche. \n",
    "\n",
    "\n",
    "### 5.2 Perceptron à une couche cachée\n",
    "#### Librairies "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T09:28:55.058629Z",
     "start_time": "2020-04-14T09:28:52.862017Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras.models as km \n",
    "import tensorflow.keras.layers as kl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T09:28:55.068108Z",
     "start_time": "2020-04-14T09:28:55.062816Z"
    }
   },
   "outputs": [],
   "source": [
    "ACTIVITIES = {\n",
    "    0: 'WALKING',\n",
    "    1: 'WALKING_UPSTAIRS',\n",
    "    2: 'WALKING_DOWNSTAIRS',\n",
    "    3: 'SITTING',\n",
    "    4: 'STANDING',\n",
    "    5: 'LAYING',\n",
    "}\n",
    "def my_confusion_matrix(Y_true, Y_pred):\n",
    "    Y_true = pd.Series([ACTIVITIES[y] for y in np.argmax(Y_true, axis=1)])\n",
    "    Y_pred = pd.Series([ACTIVITIES[y] for y in np.argmax(Y_pred, axis=1)])\n",
    "\n",
    "    return pd.crosstab(Y_true, Y_pred, rownames=['True'], colnames=['Pred'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Définition du réseau \n",
    "Une couche cachée, une couche de reformattage puis une couche de sortie à 6 classes. Le nombre de neurones (50) sur la couche cachée a été optimisé par ailleurs. Le nombre d'epochs  et la taille des batchs devraient être optimisés surtout dans le cas d'utilisation d'une carte GPU.\n",
    "\n",
    "Remarquer le nombre de paramètres à estimer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T09:28:55.273143Z",
     "start_time": "2020-04-14T09:28:55.069755Z"
    }
   },
   "outputs": [],
   "source": [
    "epochs=20\n",
    "batch_size=32\n",
    "n_hidden = 50\n",
    "\n",
    "timesteps = len(X_train[0])\n",
    "input_dim = len(X_train[0][0])\n",
    "n_classes = 6\n",
    "\n",
    "model_base_mlp =km.Sequential()\n",
    "model_base_mlp.add(kl.Dense(n_hidden, input_shape=(timesteps, input_dim),  activation = \"relu\"))\n",
    "model_base_mlp.add(kl.Reshape((timesteps*n_hidden,) , input_shape= (timesteps, n_hidden)  ))\n",
    "model_base_mlp.add(kl.Dense(n_classes, activation='softmax'))\n",
    "\n",
    "model_base_mlp.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "model_base_mlp.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T09:30:14.297438Z",
     "start_time": "2020-04-14T09:28:55.275469Z"
    }
   },
   "outputs": [],
   "source": [
    "t_start = time.time()\n",
    "model_base_mlp.fit(X_train,  Y_train_dummies, batch_size=batch_size, validation_data=(X_test, Y_test_dummies), epochs=epochs)\n",
    "t_end = time.time()\n",
    "t_learning = t_end-t_start\n",
    "\n",
    "score = model_base_mlp.evaluate(X_test, Y_test_dummies)[1] \n",
    "print(\"\\nScore With Simple MLP on Multidimensional Inertial Signals = %.2f, Learning time = %.2f secondes\" %(score*100, t_learning) )\n",
    "metadata_mlp = {\"time_learning\" : t_learning, \"score\" : score}\n",
    "base_mlp_prediction = model_base_mlp.predict(X_test)\n",
    "\n",
    "my_confusion_matrix(Y_test_dummies, base_mlp_prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q.** L'ajout d'une couche cachée permet-il d'améliorer les résultats ? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quelques efforts supplémentaires permettraient sans doute de gagner quelques points sur la précision du résultat mais... attention au sur-apprentissage si le même échantillon test est toujours utilisé. \n",
    "\n",
    "En 5ième année, dans l'UF High Dimensional and Deep Learning (HDDL), un réseau concolutionnel (CNN) sur les signaux permettra d'obtenir de meilleures performances, proches de celles obtenues sur les données métier. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
